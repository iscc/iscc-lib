# Performance Benchmarks

A core deliverable of iscc-lib is demonstrable, reproducible speedup over the Python reference
implementation (`iscc-core`). The benchmark suite measures every ISCC unit and key internal operation
across all available language targets, generates a report page, and publishes it to the project
documentation at `lib.iscc.codes/benchmarks/`.

## Design Principles

1. **Python reference is the baseline** — every result is expressed as both absolute numbers (time,
   throughput) and as a speedup factor vs `iscc-core`. This makes the value proposition immediately
   legible to adopters.
2. **Same inputs, same outputs** — benchmarks use the vendored conformance vectors and a set of
   synthetic payloads (see [Input Corpus](#input-corpus)). Every language target processes identical
   data so results are directly comparable.
3. **Streaming and single-shot** — stream-oriented operations (Data-Code, Instance-Code) are
   benchmarked at multiple input sizes to show throughput scaling. Single-shot operations (Meta-Code,
   Content-Codes, codec) are benchmarked with representative inputs at fixed sizes.
4. **No network, no disk I/O in the hot path** — benchmark inputs are pre-loaded into memory. The
   measured path is pure computation, matching how the core library is designed (bytes in, code out).
5. **Reproducible** — benchmarks run on fixed CI hardware (GitHub Actions runner class) with pinned
   tool versions. Results include runner specs (CPU model, OS, memory) so readers can contextualize
   absolute numbers.

## Benchmark Matrix

The report covers three dimensions: **operation** × **language target** × **input size**.

### Operations

| Category | Operation | Input Type | Key Metric |
|----------|-----------|------------|------------|
| **Meta-Code** | `gen_meta_code_v0` | `(name, description)` strings | Latency (μs) |
| **Content-Code: Text** | `gen_text_code_v0` | Plain text (various lengths) | Latency (μs) |
| **Content-Code: Image** | `gen_image_code_v0` | 1024 grayscale pixels (32×32) | Latency (μs) |
| **Content-Code: Audio** | `gen_audio_code_v0` | Chromaprint integer vector | Latency (μs) |
| **Content-Code: Video** | `gen_video_code_v0` | MPEG-7 frame signature sequence | Latency (μs) |
| **Content-Code: Mixed** | `gen_mixed_code_v0` | Sequence of Content-Code strings | Latency (μs) |
| **Data-Code** | `gen_data_code_v0` | Byte stream (1 KB – 100 MB) | Throughput (MB/s) |
| **Instance-Code** | `gen_instance_code_v0` | Byte stream (1 KB – 100 MB) | Throughput (MB/s) |
| **ISCC-CODE** | `gen_iscc_code_v0` | Sequence of ISCC-UNIT strings | Latency (μs) |
| **Codec: encode** | `encode_component` | Raw header + digest bytes | Latency (ns) |
| **Codec: decode** | `iscc_decode` | ISCC string | Latency (ns) |
| **Similarity** | `iscc_distance` / `iscc_similarity` | Two ISCC strings | Latency (ns) |
| **Internal: CDC** | `alg_cdc_chunks` | Byte stream (1 KB – 100 MB) | Throughput (MB/s) |
| **Internal: MinHash** | `alg_minhash_256` | Feature hash integer sequence | Latency (μs) |
| **Internal: SimHash** | `alg_simhash` | Sequence of hash digests | Latency (μs) |

### Language Targets

| Target | Label in Report | Tooling | Available From |
|--------|-----------------|---------|----------------|
| Python reference (`iscc-core`) | **Python ref** | pytest-benchmark | Phase 0 |
| Rust core (`crates/iscc`) | **Rust** | criterion | Phase 0 |
| Python bindings (`crates/iscc-py`) | **Python (Rust)** | pytest-benchmark | Phase 1 |
| Node.js bindings (`crates/iscc-node`) | **Node.js** | vitest bench | Phase 2 |
| WASM (`crates/iscc-wasm`) | **WASM** | vitest bench (browser-like) | Phase 2 |

Not all operations apply to all targets (e.g., internal algorithms like CDC and MinHash are only
benchmarked in Rust and Python ref, not through bindings). The report omits cells that don't apply
rather than showing N/A.

## Input Corpus

Benchmarks use a fixed set of inputs committed to the repository under `benchmarks/data/`. No
inputs are generated at runtime (deterministic, no RNG seeds to manage).

**Streaming inputs** (Data-Code, Instance-Code, CDC):

| Label | Size | Content |
|-------|------|---------|
| `tiny` | 1 KB | Deterministic byte pattern |
| `small` | 64 KB | Deterministic byte pattern |
| `medium` | 1 MB | Deterministic byte pattern |
| `large` | 10 MB | Deterministic byte pattern |
| `xlarge` | 100 MB | Deterministic byte pattern (generated by script, gitignored) |

The `xlarge` input is generated by a deterministic script (`benchmarks/generate_data.py`) and
gitignored to keep the repo small. CI runs the script before benchmarking. Smaller inputs are
committed directly.

**Non-streaming inputs** (Meta-Code, Content-Codes, codec):

| Operation | Input Description |
|-----------|-------------------|
| Meta-Code | Title string (50 chars) + description (500 chars) from conformance vectors |
| Text-Code | Plain text at 100, 1K, 10K, and 100K characters |
| Image-Code | 1024-element pixel array from conformance vectors |
| Audio-Code | 300-element Chromaprint vector from conformance vectors |
| Video-Code | 100-frame MPEG-7 signature sequence from conformance vectors |
| Mixed-Code | 4 Content-Code strings |
| ISCC-CODE | Full set of 4 ISCC-UNIT strings |
| Codec | 10 representative ISCC strings (various types and lengths) |

## Report Format

The benchmark report is a documentation page at `docs/benchmarks/index.md`, auto-generated by a
script (`scripts/generate_benchmark_report.py`) from JSON result files. It is rebuilt on every CI
benchmark run and published with the docs.

### Report Structure

The report page has these sections:

**1. Summary Table**

A single table showing the headline speedup of each language target vs the Python reference for each
ISCC unit type. This is the first thing readers see.

```
┌───────────────────┬─────────────┬───────────────┬──────────┬────────┐
│ Operation         │ Python ref  │ Rust          │ Py (Rust)│ Node.js│
├───────────────────┼─────────────┼───────────────┼──────────┼────────┤
│ Meta-Code         │ baseline    │ 45× faster    │ 40×      │ 42×    │
│ Text-Code (10K)   │ baseline    │ 120× faster   │ 110×     │ 115×   │
│ Image-Code        │ baseline    │ 60× faster    │ 55×      │ 58×    │
│ Audio-Code        │ baseline    │ 80× faster    │ 75×      │ 78×    │
│ Video-Code        │ baseline    │ 50× faster    │ 45×      │ 48×    │
│ Data-Code (1 MB)  │ baseline    │ 95× faster    │ 90×      │ 92×    │
│ Instance-Code     │ baseline    │ 130× faster   │ 125×     │ 128×   │
│ ISCC-CODE         │ baseline    │ 30× faster    │ 28×      │ 29×    │
└───────────────────┴─────────────┴───────────────┴──────────┴────────┘
```

> The numbers above are illustrative placeholders. iscc-sum achieves 50-130× on Data-Code and
> Instance-Code; actual numbers will vary by operation and will be filled in as benchmarks are run.

**2. Throughput Charts (streaming operations)**

For Data-Code, Instance-Code, and CDC: a chart showing throughput (MB/s) across input sizes for
each language target. This visualizes scaling behavior — a flat line means the operation is
throughput-bound (good); a declining curve suggests per-chunk overhead dominates at small sizes.

```
Data-Code Throughput (MB/s)
                    1 KB    64 KB    1 MB    10 MB    100 MB
Rust               ████    █████    █████   █████    █████    ~2,800 MB/s
Python (Rust)      ███     █████    █████   █████    █████    ~2,600 MB/s
Node.js            ███     ████     █████   █████    █████    ~2,500 MB/s
WASM               ██      ███      ████    ████     ████     ~1,200 MB/s
Python ref         █       █        █       █        █        ~  25 MB/s
```

**3. Latency Tables (non-streaming operations)**

For Meta-Code, Content-Codes, ISCC-CODE, codec, and similarity: a detailed table with absolute
latency (median, p95) and speedup factor for each language target.

```
┌──────────────────┬──────────────────┬──────────────────┬──────────┐
│ Operation        │ Python ref       │ Rust             │ Speedup  │
│                  │ median / p95     │ median / p95     │          │
├──────────────────┼──────────────────┼──────────────────┼──────────┤
│ Meta-Code        │ 450 μs / 520 μs  │  10 μs /  12 μs  │   45×    │
│ Text-Code (1K)   │ 1.2 ms / 1.4 ms  │  10 μs /  12 μs  │  120×    │
│ Text-Code (10K)  │  12 ms /  14 ms  │ 100 μs / 115 μs  │  120×    │
│ Image-Code       │ 180 μs / 210 μs  │   3 μs / 3.5 μs  │   60×    │
│ Audio-Code       │ 800 μs / 950 μs  │  10 μs /  12 μs  │   80×    │
│ Codec: encode    │  15 μs /  18 μs  │  80 ns / 100 ns  │  190×    │
│ Codec: decode    │  12 μs /  14 μs  │  60 ns /  75 ns  │  200×    │
│ Similarity       │   8 μs /  10 μs  │  25 ns /  30 ns  │  320×    │
└──────────────────┴──────────────────┴──────────────────┴──────────┘
```

**4. Internal Algorithm Details**

A collapsible section with per-algorithm benchmarks (CDC, MinHash, SimHash, DCT, WTA Hash, BLAKE3,
xxhash). These are Rust-only and Python-ref-only — not exposed through bindings. Useful for
contributors optimizing internals.

**5. Environment**

Runner specs and tool versions, recorded automatically:
- CPU model and core count
- OS and architecture
- Rust version
- Python version + iscc-core version
- Node.js version (when applicable)
- Benchmark tool versions (criterion, pytest-benchmark)

**6. Historical Trend (optional, Phase 2+)**

A link to or embedding of historical benchmark results tracked by CodSpeed or Bencher, showing
performance over time. Catches regressions and documents improvements across releases.

## Tooling

| Concern | Tool | Notes |
|---------|------|-------|
| Rust benchmarks | [criterion](https://github.com/bheisler/criterion.rs) | Statistical microbenchmarks with warm-up, outlier detection, and JSON output. Lives in `crates/iscc/benches/`. |
| Python benchmarks | [pytest-benchmark](https://pytest-benchmark.readthedocs.io/) | pytest plugin with statistical analysis and JSON output. Benchmarks `iscc-core` and `iscc-lib` Python bindings side by side. Lives in `benchmarks/python/`. |
| Node.js benchmarks | [vitest bench](https://vitest.dev/guide/features.html#benchmarking) | Vitest's built-in benchmarking (uses tinybench). Lives in `benchmarks/node/`. |
| WASM benchmarks | vitest bench (browser mode) | Same runner as Node.js but targeting WASM. |
| Report generation | Custom Python script | `scripts/generate_benchmark_report.py` reads JSON outputs from all runners, computes speedup factors, and writes `docs/benchmarks/index.md` as Markdown with tables. |
| CI tracking | [CodSpeed](https://codspeed.io/) or [Bencher](https://bencher.dev/) | Tracks performance over time in CI. Flags regressions on PRs. CodSpeed has native criterion + pytest-benchmark support. |
| Large test data | `benchmarks/generate_data.py` | Generates deterministic `xlarge` (100 MB) input. Gitignored; CI runs this before benchmarks. |

## Directory Structure

```
iscc-lib/
├── crates/iscc/benches/          # Rust criterion benchmarks
│   ├── data_code.rs               # Data-Code streaming throughput
│   ├── instance_code.rs           # Instance-Code streaming throughput
│   ├── content_codes.rs           # All Content-Code variants
│   ├── meta_code.rs               # Meta-Code latency
│   ├── codec.rs                   # Encode/decode/similarity
│   └── internals.rs               # CDC, MinHash, SimHash, etc.
├── benchmarks/
│   ├── data/                      # Committed benchmark inputs
│   │   ├── tiny.bin               # 1 KB
│   │   ├── small.bin              # 64 KB
│   │   ├── medium.bin             # 1 MB
│   │   ├── large.bin              # 10 MB
│   │   └── .gitignore             # xlarge.bin excluded
│   ├── python/
│   │   ├── bench_iscc_core.py     # Python reference benchmarks
│   │   └── bench_iscc_lib.py      # Python bindings benchmarks
│   ├── node/
│   │   └── bench_iscc.ts          # Node.js bindings benchmarks
│   └── generate_data.py           # Generates xlarge.bin deterministically
├── scripts/
│   └── generate_benchmark_report.py  # Collects JSON → Markdown report
└── docs/
    └── benchmarks/
        └── index.md               # Auto-generated report page
```

## CI Integration

Benchmarks run in CI on two triggers:

1. **On merge to main** — full benchmark suite runs on a fixed runner type. Results are committed
   as JSON artifacts, the report is regenerated, and the docs are republished. CodSpeed/Bencher
   records the data point for trend tracking.

2. **On pull request** (limited) — runs only the Rust criterion benchmarks with `--quick` mode
   to detect regressions without consuming excessive CI time. CodSpeed/Bencher posts a comment
   on the PR if any benchmark regresses beyond a configurable threshold (e.g., >5%).

**Workflow sketch:**

```yaml
# .github/workflows/benchmarks.yml
name: Benchmarks
on:
  push:
    branches: [main]
  pull_request:

jobs:
  bench-rust:
    runs-on: ubuntu-latest  # Pin to a specific runner class for consistency
    steps:
      - uses: actions/checkout@v4
      - uses: jdx/mise-action@v2
      - run: python benchmarks/generate_data.py
      - run: cargo bench --workspace -- --output-format bencher
      - uses: actions/upload-artifact@v4
        with:
          name: bench-rust
          path: target/criterion/

  bench-python:
    runs-on: ubuntu-latest
    needs: []  # Independent of Rust bench
    steps:
      - uses: actions/checkout@v4
      - uses: jdx/mise-action@v2
      - run: uv sync
      - run: uv run maturin develop --release --manifest-path crates/iscc-py/Cargo.toml
      - run: python benchmarks/generate_data.py
      - run: uv run pytest benchmarks/python/ --benchmark-json=benchmarks/results/python.json
      - uses: actions/upload-artifact@v4
        with:
          name: bench-python
          path: benchmarks/results/

  report:
    runs-on: ubuntu-latest
    needs: [bench-rust, bench-python]
    if: github.ref == 'refs/heads/main'
    steps:
      - uses: actions/checkout@v4
      - uses: actions/download-artifact@v4
      - run: python scripts/generate_benchmark_report.py
      # Report is now at docs/benchmarks/index.md — published by the docs workflow
```

## Reporting Conventions

- **Speedup factors** are computed as `python_ref_median / target_median` and displayed as `N×`.
  Values below 1× (slower than Python ref) are flagged visually.
- **Absolute numbers** use SI-appropriate units: ns for sub-microsecond, μs for microseconds, ms
  for milliseconds, MB/s for throughput.
- **Statistical rigor**: report median and p95 latency (not mean — it hides outliers). Criterion
  and pytest-benchmark both provide these natively.
- **"Python (Rust)" vs "Python ref"** — the report always includes both, making clear how much
  speedup users get just by switching `pip install iscc-core` to `pip install iscc-lib` with zero
  code changes.

## Phased Rollout

| Phase | Scope |
|-------|-------|
| **Phase 0** | Rust criterion benchmarks for implemented operations + Python reference benchmarks for the same operations via pytest-benchmark. Manual report generation. |
| **Phase 1** | Add Python bindings benchmarks. Automate report generation in CI. Publish to docs. |
| **Phase 2** | Add Node.js and WASM benchmarks. Enable CodSpeed/Bencher for historical tracking. Add throughput charts. |
| **Phase 3** | Add regression detection on PRs. Complete all operation coverage. |
